/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
  ____________       _____________
 ____    |__( )_________  __/__  /________      __
____  /| |_  /__  ___/_  /_ __  /_  __ \_ | /| / /
___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /
 _/_/  |_/_/  /_/    /_/    /_/  \____/____/|__/
[[34m2025-08-15T04:36:03.027+0000[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2025-08-15T04:36:03.045+0000[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[2025-08-15 04:36:03 +0000] [5034] [INFO] Starting gunicorn 22.0.0
[2025-08-15 04:36:03 +0000] [5034] [INFO] Listening at: http://[::]:8793 (5034)
[2025-08-15 04:36:03 +0000] [5034] [INFO] Using worker: sync
[[34m2025-08-15T04:36:03.204+0000[0m] {[34mscheduler_job_runner.py:[0m799} INFO[0m - Starting the scheduler[0m
[[34m2025-08-15T04:36:03.205+0000[0m] {[34mscheduler_job_runner.py:[0m806} INFO[0m - Processing each file at most -1 times[0m
[2025-08-15 04:36:03 +0000] [5035] [INFO] Booting worker with pid: 5035
[[34m2025-08-15T04:36:03.243+0000[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 5036[0m
[[34m2025-08-15T04:36:03.261+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[2025-08-15 04:36:03 +0000] [5037] [INFO] Booting worker with pid: 5037
[[34m2025-08-15T04:36:03.285+0000[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2025-08-15T04:36:03.450+0000] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2025-08-15T04:41:03.500+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T04:46:04.198+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T04:46:06.902+0000[0m] {[34mserve_logs.py:[0m85} WARNING[0m - The Authorization header is missing: Host: 127.0.0.1:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7
Connection: keep-alive
Referer: https://studio.firebase.google.com/
Sec-Ch-Ua: "Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"
Sec-Ch-Ua-Arch: "x86"
Sec-Ch-Ua-Bitness: "64"
Sec-Ch-Ua-Form-Factors: "Desktop"
Sec-Ch-Ua-Full-Version: "139.0.3405.86"
Sec-Ch-Ua-Full-Version-List: "Not;A=Brand";v="99.0.0.0", "Microsoft Edge";v="139.0.3405.86", "Chromium";v="139.0.7258.67"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Model: ""
Sec-Ch-Ua-Platform: "Windows"
Sec-Ch-Ua-Platform-Version: "15.0.0"
Sec-Ch-Ua-Wow64: ?0
Sec-Fetch-Dest: iframe
Sec-Fetch-Mode: navigate
Sec-Fetch-Site: cross-site
Sec-Fetch-Storage-Access: active
Sec-User-Ip: 10.20.20.205
Upgrade-Insecure-Requests: 1
X-Forwarded-Host: 8793-firebase-tims-d-eng-1755231233716.cluster-nzwlpk54dvagsxetkvxzbvslyi.cloudworkstations.dev
X-Goog-Workstations-Endpoint: workstations-43fcb701-1070-467e-a4ea-a508ea0849a3.asia-east1-c.c.monospace-10.internal.:980

.[0m
[[34m2025-08-15T04:51:04.505+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T04:56:04.941+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:01:05.252+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:06:05.538+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:11:05.838+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:16:06.116+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:21:06.301+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:24:09.870+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:09.871+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:24:09.872+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:09.877+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-08-15T05:24:09.877+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:24:09.886+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:24:16.191+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:24:16.420+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:16.436+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:24:16.437+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:17.501+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:17.575+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:17.684+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:17.775+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:24:17.911+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.extract manual__2025-08-15T05:24:08.274355+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:24:19.652+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:24:19.674+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=extract, run_id=manual__2025-08-15T05:24:08.274355+00:00, map_index=-1, run_start_date=2025-08-15 05:24:18.091867+00:00, run_end_date=2025-08-15 05:24:18.553827+00:00, run_duration=0.46196, state=success, executor_state=success, try_number=1, max_tries=0, job_id=3, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-08-15 05:24:09.873987+00:00, queued_by_job_id=2, pid=19041[0m
[[34m2025-08-15T05:24:20.247+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:20.248+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:24:20.248+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:20.252+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-08-15T05:24:20.253+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:24:20.263+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:24:23.541+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:24:23.683+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:23.696+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:24:23.698+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:24.805+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:24.876+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:24.932+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:25.021+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:24:25.091+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.transform manual__2025-08-15T05:24:08.274355+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:24:26.484+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:24:26.492+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=transform, run_id=manual__2025-08-15T05:24:08.274355+00:00, map_index=-1, run_start_date=2025-08-15 05:24:25.200632+00:00, run_end_date=2025-08-15 05:24:25.546432+00:00, run_duration=0.3458, state=success, executor_state=success, try_number=1, max_tries=0, job_id=4, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-08-15 05:24:20.250191+00:00, queued_by_job_id=2, pid=19065[0m
[[34m2025-08-15T05:24:26.912+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:26.913+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:24:26.913+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:24:08.274355+00:00 [scheduled]>[0m
[[34m2025-08-15T05:24:26.916+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-08-15T05:24:26.917+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:24:26.926+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:24:08.274355+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:24:29.751+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:24:29.903+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:29.916+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:24:29.917+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:30.810+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:30.875+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:24:30.938+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:24:31.036+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:24:31.200+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.load manual__2025-08-15T05:24:08.274355+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:24:32.765+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:24:08.274355+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:24:32.773+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=load, run_id=manual__2025-08-15T05:24:08.274355+00:00, map_index=-1, run_start_date=2025-08-15 05:24:31.378244+00:00, run_end_date=2025-08-15 05:24:31.744263+00:00, run_duration=0.366019, state=success, executor_state=success, try_number=1, max_tries=0, job_id=5, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-08-15 05:24:26.914598+00:00, queued_by_job_id=2, pid=19086[0m
[[34m2025-08-15T05:24:33.051+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun simple_etl @ 2025-08-15 05:24:08.274355+00:00: manual__2025-08-15T05:24:08.274355+00:00, state:running, queued_at: 2025-08-15 05:24:08.421687+00:00. externally triggered: True> successful[0m
[[34m2025-08-15T05:24:33.052+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=simple_etl, execution_date=2025-08-15 05:24:08.274355+00:00, run_id=manual__2025-08-15T05:24:08.274355+00:00, run_start_date=2025-08-15 05:24:09.768281+00:00, run_end_date=2025-08-15 05:24:33.052193+00:00, run_duration=23.283912, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-08-15 05:24:08.274355+00:00, data_interval_end=2025-08-15 05:24:08.274355+00:00, dag_hash=388ad5ddfe0e04736171e628bf309b88[0m
[[34m2025-08-15T05:26:06.562+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:29:33.819+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:33.820+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:29:33.820+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:33.823+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-08-15T05:29:33.823+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:29:33.831+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:29:37.296+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:29:37.479+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:37.486+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:29:37.487+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:37.918+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:37.956+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:37.989+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:38.024+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:29:38.071+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.extract manual__2025-08-15T05:29:32.458245+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:29:39.134+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:29:39.140+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=extract, run_id=manual__2025-08-15T05:29:32.458245+00:00, map_index=-1, run_start_date=2025-08-15 05:29:38.184217+00:00, run_end_date=2025-08-15 05:29:38.403019+00:00, run_duration=0.218802, state=success, executor_state=success, try_number=1, max_tries=0, job_id=6, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-08-15 05:29:33.821417+00:00, queued_by_job_id=2, pid=20662[0m
[[34m2025-08-15T05:29:39.438+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:39.439+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:29:39.439+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:39.441+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-08-15T05:29:39.441+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:29:39.450+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:29:41.443+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:29:41.519+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:41.530+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:29:41.531+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:41.971+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:42.011+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:42.043+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:42.079+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:29:42.128+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.transform manual__2025-08-15T05:29:32.458245+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:29:43.178+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:29:43.184+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=transform, run_id=manual__2025-08-15T05:29:32.458245+00:00, map_index=-1, run_start_date=2025-08-15 05:29:42.218525+00:00, run_end_date=2025-08-15 05:29:42.453737+00:00, run_duration=0.235212, state=success, executor_state=success, try_number=1, max_tries=0, job_id=7, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-08-15 05:29:39.440158+00:00, queued_by_job_id=2, pid=20677[0m
[[34m2025-08-15T05:29:43.458+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:43.459+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:29:43.459+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:29:32.458245+00:00 [scheduled]>[0m
[[34m2025-08-15T05:29:43.461+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-08-15T05:29:43.461+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:29:43.469+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:29:32.458245+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:29:45.078+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:29:45.152+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:45.161+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:29:45.162+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:45.829+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:45.868+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:29:45.901+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:29:45.935+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:29:45.979+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.load manual__2025-08-15T05:29:32.458245+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:29:47.065+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:29:32.458245+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:29:47.071+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=load, run_id=manual__2025-08-15T05:29:32.458245+00:00, map_index=-1, run_start_date=2025-08-15 05:29:46.055702+00:00, run_end_date=2025-08-15 05:29:46.270394+00:00, run_duration=0.214692, state=success, executor_state=success, try_number=1, max_tries=0, job_id=8, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-08-15 05:29:43.460165+00:00, queued_by_job_id=2, pid=20689[0m
[[34m2025-08-15T05:29:47.337+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun simple_etl @ 2025-08-15 05:29:32.458245+00:00: manual__2025-08-15T05:29:32.458245+00:00, state:running, queued_at: 2025-08-15 05:29:32.477100+00:00. externally triggered: True> successful[0m
[[34m2025-08-15T05:29:47.338+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=simple_etl, execution_date=2025-08-15 05:29:32.458245+00:00, run_id=manual__2025-08-15T05:29:32.458245+00:00, run_start_date=2025-08-15 05:29:33.783505+00:00, run_end_date=2025-08-15 05:29:47.338073+00:00, run_duration=13.554568, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-08-15 05:29:32.458245+00:00, data_interval_end=2025-08-15 05:29:32.458245+00:00, dag_hash=388ad5ddfe0e04736171e628bf309b88[0m
[[34m2025-08-15T05:31:06.816+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:31:49.141+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:49.142+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:31:49.142+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:49.144+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-08-15T05:31:49.145+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:31:49.153+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:31:51.116+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:31:51.199+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:51.209+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:31:51.210+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:51.671+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:51.711+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:51.745+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:51.782+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:31:51.828+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.extract manual__2025-08-15T05:31:48.935308+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:31:52.843+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:31:52.849+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=extract, run_id=manual__2025-08-15T05:31:48.935308+00:00, map_index=-1, run_start_date=2025-08-15 05:31:51.908020+00:00, run_end_date=2025-08-15 05:31:52.121106+00:00, run_duration=0.213086, state=success, executor_state=success, try_number=1, max_tries=0, job_id=9, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-08-15 05:31:49.143165+00:00, queued_by_job_id=2, pid=21350[0m
[[34m2025-08-15T05:31:53.134+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:53.135+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:31:53.135+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:53.137+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-08-15T05:31:53.137+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:31:53.146+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:31:54.990+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:31:55.081+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:55.090+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:31:55.091+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:55.607+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:55.648+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:55.681+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:55.722+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:31:55.773+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.transform manual__2025-08-15T05:31:48.935308+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:31:56.779+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:31:56.785+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=transform, run_id=manual__2025-08-15T05:31:48.935308+00:00, map_index=-1, run_start_date=2025-08-15 05:31:55.857043+00:00, run_end_date=2025-08-15 05:31:56.085974+00:00, run_duration=0.228931, state=success, executor_state=success, try_number=1, max_tries=0, job_id=10, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-08-15 05:31:53.135948+00:00, queued_by_job_id=2, pid=21365[0m
[[34m2025-08-15T05:31:57.054+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:57.054+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:31:57.055+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:31:48.935308+00:00 [scheduled]>[0m
[[34m2025-08-15T05:31:57.057+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-08-15T05:31:57.057+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:31:57.064+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:31:48.935308+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:31:58.729+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:31:58.806+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:58.818+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:31:58.819+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:59.385+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:59.429+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:31:59.464+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:31:59.501+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:31:59.552+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.load manual__2025-08-15T05:31:48.935308+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:32:00.641+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:31:48.935308+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:32:00.649+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=load, run_id=manual__2025-08-15T05:31:48.935308+00:00, map_index=-1, run_start_date=2025-08-15 05:31:59.633257+00:00, run_end_date=2025-08-15 05:31:59.848173+00:00, run_duration=0.214916, state=success, executor_state=success, try_number=1, max_tries=0, job_id=11, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-08-15 05:31:57.055673+00:00, queued_by_job_id=2, pid=21380[0m
[[34m2025-08-15T05:32:00.948+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun simple_etl @ 2025-08-15 05:31:48.935308+00:00: manual__2025-08-15T05:31:48.935308+00:00, state:running, queued_at: 2025-08-15 05:31:48.961591+00:00. externally triggered: True> successful[0m
[[34m2025-08-15T05:32:00.949+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=simple_etl, execution_date=2025-08-15 05:31:48.935308+00:00, run_id=manual__2025-08-15T05:31:48.935308+00:00, run_start_date=2025-08-15 05:31:49.078850+00:00, run_end_date=2025-08-15 05:32:00.949235+00:00, run_duration=11.870385, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-08-15 05:31:48.935308+00:00, data_interval_end=2025-08-15 05:31:48.935308+00:00, dag_hash=388ad5ddfe0e04736171e628bf309b88[0m
[[34m2025-08-15T05:32:20.760+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:20.760+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:32:20.761+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:20.763+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-08-15T05:32:20.763+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:32:20.771+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:32:22.621+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:32:22.700+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:22.709+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:32:22.710+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:23.199+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:23.239+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:23.289+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:23.334+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:32:23.383+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.extract manual__2025-08-15T05:32:19.498324+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:32:24.423+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:32:24.428+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=extract, run_id=manual__2025-08-15T05:32:19.498324+00:00, map_index=-1, run_start_date=2025-08-15 05:32:23.464472+00:00, run_end_date=2025-08-15 05:32:23.673605+00:00, run_duration=0.209133, state=success, executor_state=success, try_number=1, max_tries=0, job_id=12, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-08-15 05:32:20.761959+00:00, queued_by_job_id=2, pid=21473[0m
[[34m2025-08-15T05:32:24.717+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:24.717+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:32:24.717+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:24.719+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-08-15T05:32:24.719+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:32:24.727+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:32:26.426+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:32:26.499+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:26.507+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:32:26.508+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:26.922+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:26.958+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:27.030+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:27.110+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:32:27.182+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.transform manual__2025-08-15T05:32:19.498324+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:32:28.217+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:32:28.222+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=transform, run_id=manual__2025-08-15T05:32:19.498324+00:00, map_index=-1, run_start_date=2025-08-15 05:32:27.263024+00:00, run_end_date=2025-08-15 05:32:27.485828+00:00, run_duration=0.222804, state=success, executor_state=success, try_number=1, max_tries=0, job_id=13, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-08-15 05:32:24.718316+00:00, queued_by_job_id=2, pid=21488[0m
[[34m2025-08-15T05:32:28.491+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:28.491+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:32:28.492+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:32:19.498324+00:00 [scheduled]>[0m
[[34m2025-08-15T05:32:28.494+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-08-15T05:32:28.494+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:32:28.501+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:32:19.498324+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:32:30.177+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:32:30.259+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:30.271+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:32:30.272+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:30.734+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:30.774+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:32:30.808+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:32:30.847+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:32:30.894+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.load manual__2025-08-15T05:32:19.498324+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:32:31.961+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:32:19.498324+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:32:31.966+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=load, run_id=manual__2025-08-15T05:32:19.498324+00:00, map_index=-1, run_start_date=2025-08-15 05:32:30.977255+00:00, run_end_date=2025-08-15 05:32:31.212867+00:00, run_duration=0.235612, state=success, executor_state=success, try_number=1, max_tries=0, job_id=14, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-08-15 05:32:28.492777+00:00, queued_by_job_id=2, pid=21500[0m
[[34m2025-08-15T05:32:32.127+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun simple_etl @ 2025-08-15 05:32:19.498324+00:00: manual__2025-08-15T05:32:19.498324+00:00, state:running, queued_at: 2025-08-15 05:32:19.521229+00:00. externally triggered: True> successful[0m
[[34m2025-08-15T05:32:32.128+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=simple_etl, execution_date=2025-08-15 05:32:19.498324+00:00, run_id=manual__2025-08-15T05:32:19.498324+00:00, run_start_date=2025-08-15 05:32:20.721542+00:00, run_end_date=2025-08-15 05:32:32.128306+00:00, run_duration=11.406764, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-08-15 05:32:19.498324+00:00, data_interval_end=2025-08-15 05:32:19.498324+00:00, dag_hash=388ad5ddfe0e04736171e628bf309b88[0m
[[34m2025-08-15T05:36:07.071+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:36:52.258+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:52.258+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:36:52.259+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.extract manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:52.261+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2025-08-15T05:36:52.261+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:36:52.270+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'extract', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:36:54.019+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:36:54.090+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:54.097+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:36:54.098+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:36:54.579+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:54.617+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:36:54.647+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:54.681+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:36:54.727+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.extract manual__2025-08-15T05:36:50.882662+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:36:55.716+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='extract', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:36:55.722+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=extract, run_id=manual__2025-08-15T05:36:50.882662+00:00, map_index=-1, run_start_date=2025-08-15 05:36:54.806264+00:00, run_end_date=2025-08-15 05:36:55.019542+00:00, run_duration=0.213278, state=success, executor_state=success, try_number=1, max_tries=0, job_id=15, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2025-08-15 05:36:52.260136+00:00, queued_by_job_id=2, pid=22765[0m
[[34m2025-08-15T05:36:55.912+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:55.913+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:36:55.913+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.transform manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:55.915+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2025-08-15T05:36:55.915+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:36:55.925+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'transform', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:36:57.711+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:36:57.782+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:57.790+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:36:57.790+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:36:58.228+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:58.273+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:36:58.307+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:36:58.347+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:36:58.399+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.transform manual__2025-08-15T05:36:50.882662+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:36:59.495+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='transform', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:36:59.501+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=transform, run_id=manual__2025-08-15T05:36:50.882662+00:00, map_index=-1, run_start_date=2025-08-15 05:36:58.497143+00:00, run_end_date=2025-08-15 05:36:58.730325+00:00, run_duration=0.233182, state=success, executor_state=success, try_number=1, max_tries=0, job_id=16, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2025-08-15 05:36:55.914219+00:00, queued_by_job_id=2, pid=22780[0m
[[34m2025-08-15T05:36:59.767+0000[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:59.768+0000[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG simple_etl has 0/16 running and queued tasks[0m
[[34m2025-08-15T05:36:59.768+0000[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: simple_etl.load manual__2025-08-15T05:36:50.882662+00:00 [scheduled]>[0m
[[34m2025-08-15T05:36:59.770+0000[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2025-08-15T05:36:59.771+0000[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
[[34m2025-08-15T05:36:59.779+0000[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'simple_etl', 'load', 'manual__2025-08-15T05:36:50.882662+00:00', '--local', '--subdir', 'DAGS_FOLDER/simple_etl.py'][0m
/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/pendulum/tz/local_timezone.py:244 UserWarning: Unable not find any timezone configuration, defaulting to UTC.
[[34m2025-08-15T05:37:01.542+0000[0m] {[34mdagbag.py:[0m545} INFO[0m - Filling up the DagBag from /home/user/airflow/dags/simple_etl.py[0m
[[34m2025-08-15T05:37:01.618+0000[0m] {[34mtutorial_taskflow_api_virtualenv.py:[0m29} WARNING[0m - The tutorial_taskflow_api_virtualenv example DAG requires virtualenv, please install it.[0m
[[34m2025-08-15T05:37:01.627+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m40} WARNING[0m - Could not import DAGs in example_local_kubernetes_executor.py[0m
Traceback (most recent call last):
  File "/home/user/tims-d-eng/tools/airflow-venv/lib/python3.11/site-packages/airflow/example_dags/example_local_kubernetes_executor.py", line 38, in <module>
    from kubernetes.client import models as k8s
ModuleNotFoundError: No module named 'kubernetes'
[[34m2025-08-15T05:37:01.628+0000[0m] {[34mexample_local_kubernetes_executor.py:[0m41} WARNING[0m - Install Kubernetes dependencies with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:37:02.064+0000[0m] {[34mexample_python_operator.py:[0m93} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:37:02.101+0000[0m] {[34mexample_kubernetes_executor.py:[0m39} WARNING[0m - The example_kubernetes_executor example DAG requires the kubernetes provider. Please install it with: pip install apache-airflow[cncf.kubernetes][0m
[[34m2025-08-15T05:37:02.131+0000[0m] {[34mexample_python_decorator.py:[0m80} WARNING[0m - The virtalenv_python example task requires virtualenv, please install it.[0m
[[34m2025-08-15T05:37:02.164+0000[0m] {[34mworkday.py:[0m41} WARNING[0m - Could not import pandas. Holidays will not be considered.[0m
[[34m2025-08-15T05:37:02.207+0000[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: simple_etl.load manual__2025-08-15T05:36:50.882662+00:00 [queued]> on host firebase-tims-d-eng-1755231233716[0m
[[34m2025-08-15T05:37:03.351+0000[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='simple_etl', task_id='load', run_id='manual__2025-08-15T05:36:50.882662+00:00', try_number=1, map_index=-1)[0m
[[34m2025-08-15T05:37:03.356+0000[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=simple_etl, task_id=load, run_id=manual__2025-08-15T05:36:50.882662+00:00, map_index=-1, run_start_date=2025-08-15 05:37:02.284452+00:00, run_end_date=2025-08-15 05:37:02.501553+00:00, run_duration=0.217101, state=success, executor_state=success, try_number=1, max_tries=0, job_id=17, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2025-08-15 05:36:59.769470+00:00, queued_by_job_id=2, pid=22792[0m
[[34m2025-08-15T05:37:03.621+0000[0m] {[34mdagrun.py:[0m850} INFO[0m - Marking run <DagRun simple_etl @ 2025-08-15 05:36:50.882662+00:00: manual__2025-08-15T05:36:50.882662+00:00, state:running, queued_at: 2025-08-15 05:36:50.907749+00:00. externally triggered: True> successful[0m
[[34m2025-08-15T05:37:03.622+0000[0m] {[34mdagrun.py:[0m901} INFO[0m - DagRun Finished: dag_id=simple_etl, execution_date=2025-08-15 05:36:50.882662+00:00, run_id=manual__2025-08-15T05:36:50.882662+00:00, run_start_date=2025-08-15 05:36:52.217599+00:00, run_end_date=2025-08-15 05:37:03.622308+00:00, run_duration=11.404709, state=success, external_trigger=True, run_type=manual, data_interval_start=2025-08-15 05:36:50.882662+00:00, data_interval_end=2025-08-15 05:36:50.882662+00:00, dag_hash=388ad5ddfe0e04736171e628bf309b88[0m
[[34m2025-08-15T05:41:07.327+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2025-08-15T05:41:29.670+0000[0m] {[34mserve_logs.py:[0m85} WARNING[0m - The Authorization header is missing: Host: 127.0.0.1:8793
User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0
Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7
Accept-Encoding: gzip, deflate, br, zstd
Accept-Language: en-US,en;q=0.9,pt-BR;q=0.8,pt;q=0.7
Connection: keep-alive
Referer: https://studio.firebase.google.com/
Sec-Ch-Ua: "Not;A=Brand";v="99", "Microsoft Edge";v="139", "Chromium";v="139"
Sec-Ch-Ua-Arch: "x86"
Sec-Ch-Ua-Bitness: "64"
Sec-Ch-Ua-Form-Factors: "Desktop"
Sec-Ch-Ua-Full-Version: "139.0.3405.86"
Sec-Ch-Ua-Full-Version-List: "Not;A=Brand";v="99.0.0.0", "Microsoft Edge";v="139.0.3405.86", "Chromium";v="139.0.7258.67"
Sec-Ch-Ua-Mobile: ?0
Sec-Ch-Ua-Model: ""
Sec-Ch-Ua-Platform: "Windows"
Sec-Ch-Ua-Platform-Version: "15.0.0"
Sec-Ch-Ua-Wow64: ?0
Sec-Fetch-Dest: iframe
Sec-Fetch-Mode: navigate
Sec-Fetch-Site: cross-site
Sec-Fetch-Storage-Access: active
Sec-Fetch-User: ?1
Sec-User-Ip: 103.114.136.150
Upgrade-Insecure-Requests: 1
X-Forwarded-Host: 8793-firebase-tims-d-eng-1755231233716.cluster-nzwlpk54dvagsxetkvxzbvslyi.cloudworkstations.dev
X-Goog-Workstations-Endpoint: workstations-43fcb701-1070-467e-a4ea-a508ea0849a3.asia-east1-c.c.monospace-10.internal.:980

.[0m
[[34m2025-08-15T05:46:07.829+0000[0m] {[34mscheduler_job_runner.py:[0m1598} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
